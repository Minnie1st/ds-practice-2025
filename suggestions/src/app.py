import sys
import os
import grpc
from concurrent import futures
import google.generativeai as genai
import json
from dotenv import load_dotenv
import re
import uuid
import logging

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å…¨å±€ç¼“å­˜
order_data_cache = {}  # å­˜è®¢å•æ•°æ®
vector_clocks = {}     # å­˜å‘é‡æ—¶é’Ÿ

load_dotenv()

FILE = __file__ if '__file__' in globals() else os.getenv("PYTHONFILE", "")
suggestions_grpc_path = os.path.abspath(os.path.join(FILE, '../../../utils/pb/suggestions'))
sys.path.insert(0, suggestions_grpc_path)

import suggestions_pb2 as suggestions
import suggestions_pb2_grpc as suggestions_grpc


class SuggestionsServicer(suggestions_grpc.SuggestionsServicer):
    def CacheOrder(self, request, context):
        order_id = request.order_id
        vc = list(request.vector_clock)
        vc[2] += 1  # suggestions æ˜¯ç¬¬2ä¸ªä½ç½®ï¼ˆä»0å¼€å§‹è®¡æ•°ï¼‰ï¼ŒåŠ 1

        order_data_cache[order_id] = {
            'user_name': request.user_name,
            'items': [{'name': item.name, 'quantity': item.quantity} for item in request.items]
        }
        vector_clocks[order_id] = [0, 0, 0]
        print(f"Suggestions: Cached OrderID {order_id}, Vector Clock {vector_clocks[order_id]}")
        return suggestions.OrderResponse(
            success=True,
            message="Order cached",
            vector_clock=vector_clocks[order_id]
        )
    
    def GetSuggestions(self, request, context):
        order_id = request.order_id
        vc = list(request.vector_clock)  # ä»è¯·æ±‚æ‹¿å‘é‡æ—¶é’Ÿ
        vc[2] += 1  # suggestions æ˜¯ç¬¬2ä¸ªä½ç½®ï¼ˆä»0å¼€å§‹ï¼‰ï¼ŒåŠ 1
        vector_clocks[order_id] = vc  # æ›´æ–°å…¨å±€å‘é‡æ—¶é’Ÿ

        logger.info(f"Event GetSuggestions: Received GenerateSuggestions for OrderID {order_id}, Vector Clock {vc}")
        # è®¾ç½® API å¯†é’¥ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY", ""))
        
        # æ„å»ºæç¤ºï¼ˆPromptï¼‰ï¼ŒåŸºäºç”¨æˆ·è®¢å•ç”Ÿæˆæ¨è
        prompt = f"User {request.user_name} purchased the following books (no genre specified):"
        for item in request.items:
            prompt += f"\n- {item.name} (Quantity: {item.quantity})"
        prompt += """
        First, infer the most likely genre for each book based on its name. Then, recommend 3 related books, including the title and author, in strict JSON format, 
        - The output must be a flat list of 3 books in the format: [['Book1', 'Author1'], ['Book2', 'Author2'], ['Book3', 'Author3']]
        - Example output:
        ```json
        [['The Hobbit', 'J.R.R. Tolkien'], ['Dune', 'Frank Herbert'], ['1984', 'George Orwell']]
        ```"""
        print(prompt)
        logger.info(f"Received items: {[(item.name, item.quantity) for item in request.items]}")
        
        try:
            #  Gemini 2.0 Flash 
            logger.info("Calling Gemini API for Suggestion")
            model = genai.GenerativeModel("gemini-2.0-flash")
            response = model.generate_content(prompt)
            logger.info(f"ğŸ” Gemini API Raw Response: {response.text}")
            
            
            #  response.text å¯ç”¨ï¼ˆå¤„ç†å¯èƒ½çš„ None æˆ–å¼‚å¸¸ï¼‰
            if not response.text or not response.text.strip():
                raise ValueError("Received empty response from Gemini API")
            
            json_match = re.search(r"```json\s*([\s\S]*?)\s*```", response.text, re.DOTALL)
            if json_match:
                books_json = json_match.group(1).strip()  # æå– JSON å†…å®¹
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾å¯èƒ½çš„ JSON éƒ¨åˆ†
                books_json = re.sub(r"[^$$  $$\{\},:\"'\w\s-]", "", response.text.strip())
                # è¿›ä¸€æ­¥æ¸…ç†ï¼Œç¡®ä¿åªä¿ç•™ JSON ç›¸å…³å­—ç¬¦
                books_json = re.sub(r"\s+", " ", books_json).strip()

            # è¿›ä¸€æ­¥æ£€æŸ¥ books_json æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ JSON
            try:
                books_list = json.loads(books_json)
            except json.JSONDecodeError as e:
                logger.info(f"Error decoding JSON: {e}")
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¹¦å•
                books_list = []

            # å¦‚æœ books_list ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æ¨è
            if not books_list:
                logger.info("No valid book recommendations found, returning defaults.")
                books_list = [
                    ["Python Crash Course", "Eric Matthes"],
                    ["Introduction to Algorithms", "Thomas H. Cormen"]
                ]

            # è½¬æ¢ä¸º protobuf æ ¼å¼çš„ä¹¦ç±åˆ—è¡¨
            suggested_books = [
                suggestions.Book( 
                    title=book[0],
                    author=book[1]
                )
                for i, book in enumerate (books_list)
            ]
            
            logger.info(f"Event f: OrderID {order_id}, Generated {len(suggested_books)} suggestions, Vector Clock {vc}")
            return suggestions.OrderResponse(
                success=True,
                message="Suggestions generated",
                suggested_books=suggested_books[:3],  # æœ€å¤š3æœ¬
                vector_clock=vc
            )

        except Exception as e:
            logger.error(f"Error calling Gemini API: {e}")
            default_books = [
                suggestions.Book(title="Python Crash Course", author="Eric Matthes"),
                suggestions.Book(title="Introduction to Algorithms", author="Thomas H. Cormen")
            ]
            return suggestions.OrderResponse(
                success=False,
                message=f"Failed to generate suggestions: {str(e)}",
                suggested_books=default_books,
                vector_clock=vc
            )
    
    def ClearOrder(self, request, context):
        order_id = request.order_id
        final_vc = list(request.final_vector_clock)  # VCf
        local_vc = vector_clocks.get(order_id, [0, 0, 0])  # æœ¬åœ° VC

        # æ£€æŸ¥ VC <= VCf
        is_valid = all(local_vc[i] <= final_vc[i] for i in range(len(local_vc)))
        if is_valid:
            # æ¸…ç†æ•°æ®
            if order_id in order_data_cache:
                del order_data_cache[order_id]
            if order_id in vector_clocks:
                del vector_clocks[order_id]
            print(f"Suggestions: Cleared OrderID {order_id}, Local VC {local_vc}, Final VC {final_vc}")
            return suggestions.ClearResponse(
                success=True,
                message="Order data cleared"
            )
        else:
            print(f"Suggestions: Error for OrderID {order_id}, Local VC {local_vc} > Final VC {final_vc}")
            return suggestions.ClearResponse(
                success=False,
                message="Vector clock mismatch"
            )
        
def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    suggestions_grpc.add_SuggestionsServicer_to_server(
        SuggestionsServicer(), server
    )
    server.add_insecure_port('[::]:50053')  # ç›‘å¬ç«¯å£ 50053
    server.start()
    print("Suggestions service started on port 50053")
    server.wait_for_termination()

if __name__ == '__main__':
    serve()